%!TEX root = ../proyecto.tex
%\thispagestyle{empty}
%\cleardoublepage

%\thispagestyle{empty}

%\input{portada/portada_2}



\cleardoublepage
\thispagestyle{empty}

\begin{center}
{\large\bfseries Desarrollo e Implementación de modelos paralelos de Soft Computing en CUDA.}\\
\end{center}
\begin{center}
David Criado Ramón\\
\end{center}

%\vspace{0.7cm}
\noindent{\textbf{Palabras clave}: Soft Computing, CUDA, GPU, paralelo, mapa auto-organizado, Spark, Kohonen, árbol de decisión, CART, CUDT, Python, Numba, SUSY, reducción, scan.}\\

\vspace{0.7cm}
\noindent{\textbf{Resumen}}\\

El objetivo de este proyecto es la paralelización de varios algoritmos de \textit{Soft Computing} usando la tecnología propietaria de \textit{NVIDIA} para sus tarjetas gráficas, \textit{CUDA}. Dos algoritmos fueron seleccionados para su desarrollo: el mapa auto-organizado de \textit{Kohonen} y el árbol de decisión \textit{CART}. Además, para simplificar el proceso de desarrollo nos apoyamos en el \textit{framework} para computación en clúster \textit{Apache Spark}, que nos ha permitido desarrollar una solución viable para ser utilizada bien en una única máquina con un dispositivo \textit{CUDA} o en un clúster con múltiples máquinas, cada una con su dispositivo CUDA correspondiente.\\

En el caso del mapa auto-organizado de Kohonen planteamos una solución basada en la primitiva paralela de la reducción y el uso de operaciones atómicas. Además, para evitar \textit{overheads} debidos al lanzamiento de múltiples \textit{kernels}, limitamos el tamaño del mapa de salida a 1024 neuronas. \\

En el caso del árbol de decisión \textit{CART}, seguimos una alternativa similar a CUDT, basándonos en la primitiva paralela de suma acumulada o \textit{scan} y limitando los problemas a resolver a clasificación binaria, añadiendo algunas técnicas extra, como los \textit{streams}, para obtener resultados ligeramente mejores en tiempo de ejecución. \\

Para comprobar el rendimiento de ambos modelos hemos utilizado la base de datos \textit{SUSY}, compuesta por 5 millones de muestras con 18 características y una clase binaria correspondiente y, haciendo uso de \textit{NumPy}, hemos desarrollado las versiones equivalentes utilizando la CPU. En las pruebas realizadas llegamos a obtener un tiempo de ejecución hasta 28 más rápido para el mapa auto-organizado de \textit{Kohonen} con el dispositivo \textit{CUDA}. Sin embargo, para el árbol de decisión desarrollado, aunque conseguimos tiempos de ejecución más rápidos, obtenemos resultados mucho más moderados, con tiempos de ejecución hasta 1,5 veces más rápidos.


\thispagestyle{empty}


\begin{center}
{\large\bfseries Development and Implementation of Soft Computing parallel models using CUDA.}\\
\end{center}
\begin{center}
David Criado Ramón\\
\end{center}

%\vspace{0.7cm}
\noindent{\textbf{Keywords}: Soft Computing, CUDA, GPU, parallel, self-organizing map, Spark, Kohonen, decision tree, CART, CUDT, Python, Numba, SUSY, reduction, scan.}\\

\vspace{0.7cm}
\noindent{\textbf{Abstract}}\\

In this project, we parallelize two Soft Computing models using CUDA: self-organizing maps, a competitive and unsupervised learning neuronal network and CART decision trees, an exhaustive search decision tree algorithm suitable for classification and regression tasks (though we are limiting here our implementation, based on GPU, to binary classification problems).\\

Both models are developed using Python and \textit{Apache Spark}. \textit{Python} allows us to implement our code faster via \textit{Numba} than using the more traditional approach with\textit{C++} and \textit{CUDA} and, thereby, it offers an easy integration with Spark. On the other hand, \textit{Spark} offers an easy way to read CSV files, manageable parameters to solve scalability issues (executors, number of cores, étc.) and the capability to carry out only one implementation, which is  suitable for execution either on one machine. In order to compare GPU performance vs CPU, we use NumPY and Spark to create CPU-based implementations that work like the GPU ones.\\

The self-organizing map (SOM), proposed by Kohonen in the early 80s, is an unsupervised learning algorithm suitable for clustering and dimensionality reduction, among others. Our solution, based on the batch version of the SOM, uses Spark's \textit{mapPartition} transformation in order to distribute work among all the partitions of the RDD. Each iteration, on each partition, our implementation uses the Euclidean distance to calculate distance between the neurons, as well as the parallel reduction primitive in order to find the closest neuron (BMU) and updates the weights structure using atomic operations. Furthermore, we limit the number of neurons on the output map to 1024 in order to avoid overheads by multiple kernel launches. We achieve an speedup of almost 28 times compared with the CPU implementation on the selected dataset, SUSY, composed by 5 million instances with 18 features.\\

The development of a decision tree learning algorithm proved to be much more challenging. Being based on CART, we decided to develop a solution similar to CUDT, some small tweaks like common on-line pruning techniques: changing depth and number of samples assigned to the leaves of trees and the use of streams to allow concurrent evaluation of nodes in the GPU. In this algorithm implementation, by using Python limits the implementation of the algorithm since Numba does not support CUDA dynamic
parallelism, thenforcing us to launch multiple kernels instead. Also, being based on CUDT, our implementation only solves binary classification problems and uses the parallel scan primitive, which we implemented using warp constraint. We incorporate Spark to this model by creating a random forest of these trees. This will help to prevent overfitting and will be, in fact, faster than using Spark to create a single tree,	 since the latter would create a serious communication overhead between the RDD's partitions. To test our implementation, we use SUSY dataset (5 million samples, 18 features, 1 binary class label). Our GPU implementation was able to improve the CPU one, however, the best speedup we were able to obtain in our system was only 1.5 times faster than the second one, although parameters like the maximum depth, the number of trees in the random forest or the number of samples of the dataset may have heavily impacted the results obtained.

\newpage
\thispagestyle{empty}

\noindent\rule[-1ex]{\textwidth}{2pt}\\[4.5ex]

Yo, \textbf{David Criado Ramón}, alumno de la titulación Grado en Ingeniería Informática de la \textbf{Escuela Técnica Superior
de Ingenierías Informática y de Telecomunicación de la Universidad de Granada}, con DNI 26254133-R, autorizo la
ubicación de la siguiente copia de mi Trabajo Fin de Grado en la biblioteca del centro para que pueda ser
consultada por las personas que lo deseen.

\vspace{6cm}

\noindent Fdo: David Criado Ramón

\vspace{2cm}

\begin{flushright}
Granada a 3 de septiembre de 2019.
\end{flushright}


\chapter*{}
\thispagestyle{empty}
\noindent\rule[-1ex]{\textwidth}{1pt}\\[4.5ex]
D. \textbf{Manuel Capel Tuñón}, Profesor del Departamento de Lenguajes y Sistemas Informáticos de la Universidad de Granada.

\vspace{0.5cm}

D. \textbf{María del Carmen Pegalajar Jiménez}, Profesora del Departamento de Ciencias de la Computación e Inteligencia Artificial de la Universidad de Granada.


\vspace{0.5cm}

\textbf{Informan:}

\vspace{0.5cm}

Que el presente trabajo, titulado \textit{\textbf{Desarrollo e Implementación de modelos paralelos de Soft Computing en CUDA}},
ha sido realizado bajo su supervisión por \textbf{David Criado Ramón}, y autorizamos la defensa de dicho trabajo ante el tribunal
que corresponda.

\vspace{0.5cm}

Y para que conste, expiden y firman el presente informe en Granada a 3 de septiembre de 2019.

\vspace{1cm}

\textbf{Los directores:}

\vspace{4cm}

\noindent \textbf{Manuel I. Capel Tuñón \ \ \ \ \ María del Carmen Pegalajar Jiménez}

\chapter*{Agradecimientos}
\thispagestyle{empty}

       \vspace{1cm}


A Rubén, por estar siempre apoyándome.